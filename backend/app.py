from flask import Flask, request, jsonify, send_from_directory
from flask_cors import CORS
import requests
import os

app = Flask(__name__)
CORS(app)

# Function to Get AI Response from an External API
def get_ai_response(user_message):
    """
    Placeholder Function for AI Integration.
    This function handles the API request to an AI service.

    Parameters:
    user_message (str): The message input by the user.

    Returns:
    str: The response generated by the AI service.
    """
    # Placeholder logic for AI call using LM Studio or any other service.
    lm_studio_url = os.getenv('AI_API_URL', 'http://YOUR_AI_SERVICE_URL_HERE')
    model_name = os.getenv('AI_MODEL_NAME', 'YOUR_MODEL_NAME_HERE')

    try:
        response = requests.post(lm_studio_url, json={
            'model': model_name,  # Replace with your model's API identifier
            'messages': [{'role': 'user', 'content': user_message}],
            'max_tokens': 100,
        })
        
        response_data = response.json()

        if response.status_code == 200 and 'choices' in response_data:
            # Return the content of the AI's response
            return response_data['choices'][0]['message']['content']
    except requests.exceptions.RequestException as e:
        # Return error message if there was an issue contacting the AI service
        return f"Error contacting AI service: {str(e)}"

    # Return a default response if the AI service fails or is unavailable
    return f"Echo: {user_message}"

# Basic Route for Health Check
@app.route('/')
def index():
    """
    Health Check Route.
    This route can be used to verify if the backend server is running properly.
    """
    return jsonify({'message': 'Backend is working!'})

# Chat Route to Handle User Messages
@app.route('/api/chat', methods=['POST'])
def chat():
    """
    Chat Route to Handle User Messages.
    Receives a user message via POST request, passes it to the AI model, 
    and returns the AI's response.

    Request JSON Example:
    {
        "message": "Hello"
    }

    Returns:
    JSON containing the AI response:
    {
        "response": "Hi! How can I assist you today?"
    }
    """
    # Get the user's message from the incoming JSON payload
    user_message = request.json.get('message')

    if not user_message:
         return jsonify({'error': 'Message is required'}), 400

    # Get AI response using the placeholder function
    ai_reply = get_ai_response(user_message)

    # Return the AI's response
    if ai_reply:
        return jsonify({'response': ai_reply})
    else:
        return jsonify({'error': 'Failed to generate response'}), 500

# Route to Serve Static Frontend Files
@app.route('/<path:filename>')
def serve_frontend(filename):
    """
    Serve Static Frontend Files.
    This route serves static files from the frontend directory.

    Parameters:
    filename (str): The name of the file to serve.

    Returns:
    File: The requested static file.
    """
    return send_from_directory('../frontend', filename)

# Main entry point to run the server
if __name__ == '__main__':
    # Run the server in debug mode, accessible from any IP address
    app.run(debug=True, host='0.0.0.0')
